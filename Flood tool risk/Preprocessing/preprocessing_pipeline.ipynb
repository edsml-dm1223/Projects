{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the current directory to PYTHONPATH\n",
    "sys.path.append(os.getcwd())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##General preprocessing pipeline\n",
    "\n",
    "class FloodRiskPreprocessingPipeline:\n",
    "    \"\"\"Enhanced preprocessing pipeline for flood risk df.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.one_hot_encoder = None\n",
    "        self.scaler = None\n",
    "        self.imputers = {}\n",
    "        self.log = []  # Log actions for debugging and reporting\n",
    "\n",
    "    def drop_duplicates(self, df):\n",
    "        df.drop_duplicates(inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_postcode(postcode):\n",
    "        \"\"\"Standardizes a postcode to the format 'SW1A 1AA'.\"\"\"\n",
    "        if isinstance(postcode, str):\n",
    "            postcode = postcode.strip().upper()\n",
    "            match = re.match(r'^([A-Z]{1,2}[0-9][A-Z0-9]?)(\\s*?)([0-9][A-Z]{2})$', postcode)\n",
    "            if match:\n",
    "                return f\"{match.group(1)} {match.group(3)}\"\n",
    "        return postcode\n",
    "\n",
    "    def merge_dfsets(self, df, sector_data=None, station_data=None, district_data=None):\n",
    "        \"\"\"Merges additional datasets into the main DataFrame.\"\"\"\n",
    "        # Merge sector data\n",
    "        if sector_data is not None and \"sector\" in df.columns:\n",
    "            df = df.merge(sector_data, on=\"sector\", how=\"left\")\n",
    "            self.log.append(\"Merged 'sector_data' into the main dataset.\")\n",
    "\n",
    "        # Merge district data\n",
    "        if district_data is not None and \"postcodeDistrict\" in df.columns:\n",
    "            df = df.merge(district_data, on=\"postcodeDistrict\", how=\"left\")\n",
    "            self.log.append(\"Merged 'district_data' into the main dataset.\")\n",
    "\n",
    "        # Add distance to nearest station(Improves geospatial understanding of flood risks by correlating risk with monitored data.)\n",
    "        if station_data is not None:\n",
    "            station_coords = station_data[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "            postcode_coords = df[[\"northing\", \"easting\"]].to_numpy()\n",
    "            distances = cdist(postcode_coords, station_coords, metric=\"euclidean\")\n",
    "            df[\"distance_to_station\"] = distances.min(axis=1)\n",
    "            self.log.append(\"Added 'distance_to_station' feature using station data.\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    #computer the nearest point based on 3D \n",
    "\n",
    "    def handle_missing_data(self, df, categorical_columns, numeric_columns, method=\"median\"):\n",
    "        \"\"\"Handles missing data using the specified imputation strategy.\"\"\"\n",
    "        # Impute categorical columns with the mode\n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "                df[col] = mode_imputer.fit_transform(df[[col]])\n",
    "                self.imputers[col] = mode_imputer\n",
    "                self.log.append(f\"Imputed missing values in '{col}' using mode.\")\n",
    "\n",
    "        # Impute numeric columns\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                imputer = SimpleImputer(strategy=method)\n",
    "                df[col] = imputer.fit_transform(df[[col]])\n",
    "                self.imputers[col] = imputer\n",
    "                self.log.append(f\"Imputed missing values in '{col}' using {method} strategy.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def scale_numeric_features(self, df, numeric_columns, scaling_type=\"standard\"):\n",
    "        \"\"\"Scales numeric features using the specified scaler.\"\"\"\n",
    "        if scaling_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaling_type == \"minmax\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif scaling_type == 'robustscaler':\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling_type. Use 'standard' or 'minmax'.\")\n",
    "\n",
    "        numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "        if numeric_columns:\n",
    "            df[numeric_columns] = self.scaler.fit_transform(df[numeric_columns])\n",
    "            self.log.append(f\"Scaled numeric columns: {numeric_columns} using {scaling_type} scaling.\")\n",
    "        return df\n",
    "\n",
    "    def feature_engineering(self, df, sector_data=None, station_data=None):\n",
    "        \"\"\"Adds derived features like proximity risk and population density.\"\"\"\n",
    "        # Calculate proximity risk\n",
    "        if \"distanceToWatercourse\" in df.columns and \"elevation\" in df.columns:\n",
    "            df[\"proximity_risk\"] = df[\"distanceToWatercourse\"] / (df[\"elevation\"] + 1)\n",
    "            self.log.append(\"Added 'proximity_risk' feature.\")\n",
    "\n",
    "        # Add population density from sector data\n",
    "        if sector_data is not None and \"sector\" in df.columns:\n",
    "            sector_columns = [\"sector\", \"population\", \"households\", \"numberOfPostcodeUnits\"]\n",
    "            df[\"population_density\"] = df[\"population\"] / df[\"households\"]\n",
    "            self.log.append(\"Added 'population_density' feature from sector data.\")\n",
    "\n",
    "\n",
    "        return df\n",
    "    \n",
    "   \n",
    "    def interaction_general(self, df):\n",
    "        # Initialize label encoders\n",
    "        label_encoder_soil = LabelEncoder()\n",
    "        label_encoder_watercourse = LabelEncoder()\n",
    "\n",
    "        # Encode 'soilType' and 'nearestWatercourse'\n",
    "        df['soilType_encoded'] = label_encoder_soil.fit_transform(df['soilType'])\n",
    "        df['nearestWatercourse_encoded'] = label_encoder_watercourse.fit_transform(df['nearestWatercourse'])\n",
    "\n",
    "        # Calculate bins for elevation and distanceToWatercourse\n",
    "        percentiles = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "        bins_elevation = df['elevation'].quantile(percentiles).values\n",
    "        bins_distance = df['distanceToWatercourse'].quantile(percentiles).values\n",
    "\n",
    "        # Define bin labels\n",
    "        bin_labels = ['Low-Mid', 'Mid', 'Mid-High', 'High']\n",
    "\n",
    "         # Perform binning\n",
    "        df['elevation_category'] = pd.cut(df['elevation'], bins=bins_elevation, labels=bin_labels, include_lowest=True)\n",
    "        df['distanceToWatercourse_category'] = pd.cut(df['distanceToWatercourse'], bins=bins_distance, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "        # Combine categorical columns for interaction terms\n",
    "        df['soilType/Elevation'] = df['soilType_encoded'].astype(str) + '/' + df['elevation_category'].astype(str)\n",
    "        df['distanceToWatercourse/nearestWatercourse'] = df['distanceToWatercourse_category'].astype(str) + '/' + df['nearestWatercourse_encoded'].astype(str)\n",
    "  \n",
    "\n",
    "        # Apply label encoding to interaction columns\n",
    "        interaction_encoders = {col: LabelEncoder() for col in ['soilType/Elevation', 'distanceToWatercourse/nearestWatercourse' ]}\n",
    "        for col, encoder in interaction_encoders.items():\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "    # Concatenate the interaction DataFrame back to the original DataFrame\n",
    "\n",
    "        # Drop unnecessary intermediate encoded columns\n",
    "        df.drop(columns=['soilType_encoded', 'nearestWatercourse_encoded', 'elevation_category', 'distanceToWatercourse_category'], inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, df, categorical_columns=None, numeric_columns=None, scaling_type=\"standard\",\n",
    "                   imputation_method=\"median\", sector_data=None, station_data=None, district_data=None):\n",
    "        \"\"\"\n",
    "        Executes the complete preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        if categorical_columns is None:\n",
    "            categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        if numeric_columns is None:\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Standardize postcodes\n",
    "        if \"postcode\" in df.columns:\n",
    "            df[\"postcode\"] = df[\"postcode\"].apply(self.standardize_postcode)\n",
    "            self.log.append(\"Standardized 'postcode' column.\")\n",
    "\n",
    "        # Merge supporting datasets\n",
    "        df = self.merge_datasets(df, sector_data, station_data, district_data)\n",
    "\n",
    "        # Handle missing data\n",
    "        df = self.handle_missing_data(df, categorical_columns, numeric_columns, method=imputation_method)\n",
    "\n",
    "        # Encode categorical features\n",
    "        df = self.encode_categorical_features(df, categorical_columns)\n",
    "\n",
    "        # Feature engineering\n",
    "        df = self.feature_engineering(df, sector_data, station_data)\n",
    "\n",
    "        # Scale numeric features\n",
    "        df = self.scale_numeric_features(df, numeric_columns, scaling_type=scaling_type)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generates a summary of preprocessing actions.\"\"\"\n",
    "        return \"\\n\".join(self.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to convert lat/lon/elevation to Cartesian coordinates\n",
    "def lat_lon_to_cartesian(lat, lon, elevation, earth_radius=6371):\n",
    "    \"\"\"Converts latitude, longitude, and elevation to Cartesian coordinates.\"\"\"\n",
    "    lat_rad = np.radians(lat)\n",
    "    lon_rad = np.radians(lon)\n",
    "    r = earth_radius + (elevation / 1000.0)\n",
    "    x = r * np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = r * np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = r * np.sin(lat_rad)\n",
    "    return x, y, z\n",
    "\n",
    "class FloodRiskPreprocessingPipeline:\n",
    "    \"\"\"Enhanced preprocessing pipeline for flood risk datasets.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "        self.log = []\n",
    "    \n",
    "    def drop_duplicates(self, df):\n",
    "        \"\"\"Drops duplicate rows from the dataset.\"\"\"\n",
    "        original_count = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        dropped_count = original_count - len(df)\n",
    "        self.log.append(f\"Dropped {dropped_count} duplicate rows.\")\n",
    "        return df\n",
    "    @staticmethod\n",
    "    \n",
    "    def standardize_postcode(postcode):\n",
    "        \"\"\"Standardizes a postcode to the format 'SW1A 1AA'.\"\"\"\n",
    "        if isinstance(postcode, str):\n",
    "            postcode = postcode.strip().upper()\n",
    "            match = re.match(r'^([A-Z]{1,2}[0-9][A-Z0-9]?)(\\s*?)([0-9][A-Z]{2})$', postcode)\n",
    "            if match:\n",
    "                return f\"{match.group(1)} {match.group(3)}\"\n",
    "        return postcode\n",
    "\n",
    "    def preprocess_postcode(self, df):\n",
    "        \"\"\"Splits the 'postcode' column into 'postcodeSector' and 'postcodeDistrict'.\"\"\"\n",
    "        if \"postcode\" in df.columns:\n",
    "            df[\"postcodeSector\"] = df[\"postcode\"].str.split(\" \").str[0]\n",
    "            df[\"postcodeDistrict\"] = df[\"postcode\"].str.extract(r'^([A-Z]{1,2}\\d{1,2}\\s?\\d?)')[0]\n",
    "            self.log.append(\"Extracted 'postcodeSector' and 'postcodeDistrict' from 'postcode'.\")\n",
    "        return df\n",
    "\n",
    "    def merge_sector_data(self, df, sector_data):\n",
    "        \"\"\"Merges sector-level data.\"\"\"\n",
    "        if \"postcodeSector\" in df.columns:\n",
    "            df = df.merge(sector_data, on=\"postcodeSector\", how=\"left\")\n",
    "            self.log.append(\"Merged 'sector_data' into the main dataset.\")\n",
    "        return df\n",
    "\n",
    "    def merge_district_data(self, df, district_data):\n",
    "        \"\"\"Merges district-level data.\"\"\"\n",
    "        if \"postcodeDistrict\" in df.columns:\n",
    "            df = df.merge(district_data, on=\"postcodeDistrict\", how=\"left\")\n",
    "           # self.log.append(\"Merged 'district_data' into the main dataset.\")\n",
    "        return df\n",
    "    \n",
    "    def increase_negative_for_log(self, df):\n",
    "        for col in df.select_dtypes(include = np.number).columns:\n",
    "             min_val = df[col].min()\n",
    "             if min_val <= 0:\n",
    "                shift_value = abs(min_val) + 0.001\n",
    "                df[col] += shift_value\n",
    "                self.log.append(f\"Shifted '{col}' by {shift_value} to handle negatives.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def lat_long_compute(self, df):\n",
    "        from flood_tool.geo import get_gps_lat_long_from_easting_northing\n",
    "        coordinates_lat = get_gps_lat_long_from_easting_northing(df[])\n",
    "        coordinates_df = pd.DataFrame({\n",
    "            'latitude': coordinates_lat[0],\n",
    "            'longitude': coordinates_lat[1]\n",
    "        })\n",
    "        df= pd.concat([df, coordinates_df], axis=1)\n",
    "        df.drop(columns = ['easting', 'northing'], inplace = True)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "    def impute_watercourse_3d(self, df, lat_col=\"latitude\", lon_col=\"longitude\", elev_col=\"elevation\", watercourse_col=\"nearestWatercourse\"):\n",
    "        \"\"\"Imputes missing watercourse values using 3D Cartesian distance.\"\"\"\n",
    "        df[\"cartesian\"] = df.apply(lambda row: lat_lon_to_cartesian(row[lat_col], row[lon_col], row[elev_col]), axis=1)\n",
    "        known = df[df[watercourse_col].notna()]\n",
    "        unknown = df[df[watercourse_col].isna()]\n",
    "        tree = KDTree(np.array(known[\"cartesian\"].tolist()))\n",
    "        distances, indices = tree.query(np.array(unknown[\"cartesian\"].tolist()))\n",
    "        unknown[watercourse_col] = known.iloc[indices][watercourse_col].values\n",
    "        self.log.append(f\"Imputed missing 'nearestWatercourse' using 3D Cartesian distance.\")\n",
    "        return pd.concat([known, unknown]).drop(columns=[\"cartesian\"])\n",
    "\n",
    "    def handle_missing_numeric(self, df, numeric_col=\"medianPrice\"):\n",
    "        \"\"\"Handles missing numeric data by replacing with the median.\"\"\"\n",
    "        if numeric_col in df.columns:\n",
    "            median_value = df[numeric_col].median()\n",
    "            df[numeric_col].fillna(median_value, inplace=True)\n",
    "            self.log.append(f\"Imputed missing values in '{numeric_col}' with its median value ({median_value}).\")\n",
    "        return df\n",
    "\n",
    "    def scale_numeric_features(self, df, numeric_columns, scaling_type=\"standard\"):\n",
    "        \"\"\"Scales numeric features using the specified scaler.\"\"\"\n",
    "        if scaling_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaling_type == \"minmax\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif scaling_type == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling_type. Use 'standard', 'minmax', or 'robust'.\")\n",
    "\n",
    "        numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "        if numeric_columns:\n",
    "            df[numeric_columns] = self.scaler.fit_transform(df[numeric_columns])\n",
    "            self.log.append(f\"Scaled numeric columns: {numeric_columns} using {scaling_type} scaling.\")\n",
    "        return df\n",
    "    def feature_engineering(self, df, sector_data=None, station_data=None):\n",
    "        \"\"\"Adds derived features like proximity risk and population density.\"\"\"\n",
    "        # Calculate proximity risk\n",
    "        if \"distanceToWatercourse\" in df.columns and \"elevation\" in df.columns:\n",
    "            df[\"proximity_risk\"] = df[\"distanceToWatercourse\"] / (df[\"elevation\"] + 1)\n",
    "            self.log.append(\"Added 'proximity_risk' feature.\")\n",
    "\n",
    "        # Add population density from sector data\n",
    "        if sector_data is not None and \"households\" in sector_data.columns and \"population\" in sector_data.columns:\n",
    "            df[\"population_density\"] = sector_data[\"population\"] / sector_data[\"households\"]\n",
    "            self.log.append(\"Added 'population_density' feature from sector data.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def interaction_general(self, df):\n",
    "        \"\"\"Generates interaction features based on categorical and numeric data.\"\"\"\n",
    "        # Initialize label encoders\n",
    "        label_encoder_soil = LabelEncoder()\n",
    "        label_encoder_watercourse = LabelEncoder()\n",
    "\n",
    "        # Encode 'soilType' and 'nearestWatercourse'\n",
    "        df['soilType_encoded'] = label_encoder_soil.fit_transform(df['soilType'])\n",
    "        df['nearestWatercourse_encoded'] = label_encoder_watercourse.fit_transform(df['nearestWatercourse'])\n",
    "\n",
    "        # Binning for elevation and distance to watercourse\n",
    "        bins_elevation = pd.qcut(df['elevation'], q=4, labels=[\"Low\", \"Mid\", \"High\", \"Very High\"])\n",
    "        bins_distance = pd.qcut(df['distanceToWatercourse'], q=4, labels=[\"Low\", \"Mid\", \"High\", \"Very High\"])\n",
    "\n",
    "        # Interaction terms\n",
    "        df['soilType/Elevation'] = df['soilType_encoded'].astype(str) + '/' + bins_elevation.astype(str)\n",
    "        df['distanceToWatercourse/nearestWatercourse'] = bins_distance.astype(str) + '/' + df['nearestWatercourse_encoded'].astype(str)\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        df.drop(columns=['soilType_encoded', 'nearestWatercourse_encoded'], inplace=True)\n",
    "\n",
    "        self.log.append(\"Added interaction terms and encoded features.\")\n",
    "        return df\n",
    "\n",
    "    def preprocess(self, df, sector_data=None, district_data=None,\n",
    "                   lat_col=\"latitude\", lon_col=\"longitude\", elev_col=\"elevation\",\n",
    "                   watercourse_col=\"nearestWatercourse\", numeric_col=\"medianPrice\",\n",
    "                   numeric_columns=None, scaling_type=\"standard\"):\n",
    "        \"\"\"Executes the complete preprocessing pipeline.\"\"\"\n",
    "        \n",
    "        # Drop duplicates\n",
    "        df = self.drop_duplicates(df)\n",
    "        \n",
    "        # Standardize postcodes and extract derived columns\n",
    "        if \"postcode\" in df.columns:\n",
    "            df[\"postcode\"] = df[\"postcode\"].apply(self.standardize_postcode)\n",
    "            self.log.append(\"Standardized 'postcode' column.\")\n",
    "        df = self.preprocess_postcode(df)\n",
    "\n",
    "        # Merge sector and district data\n",
    "        if sector_data is not None:\n",
    "            df = self.merge_sector_data(df, sector_data)\n",
    "        if district_data is not None:\n",
    "            df = self.merge_district_data(df, district_data)\n",
    "\n",
    "        # Impute missing categorical and numeric data\n",
    "        if watercourse_col in df.columns:\n",
    "            df = self.impute_watercourse_3d(df, lat_col, lon_col, elev_col, watercourse_col)\n",
    "        df = self.handle_missing_numeric(df, numeric_col=numeric_col)\n",
    "\n",
    "         # Feature engineering\n",
    "        df = self.feature_engineering(df, sector_data, station_data)\n",
    "\n",
    "        # Generate interaction terms\n",
    "        df = self.interaction_general(df)\n",
    "\n",
    "        # Scale numeric features\n",
    "        if numeric_columns is None:\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        df = self.scale_numeric_features(df, numeric_columns, scaling_type=scaling_type)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generates a summary of preprocessing actions.\"\"\"\n",
    "        return \"\\n\".join(self.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Final\n",
    "\n",
    "class FloodRiskPreprocessingPipeline:\n",
    "    \"\"\"Simplified preprocessing pipeline for flood risk datasets.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "\n",
    "    def drop_duplicates(self, df):\n",
    "        \"\"\"Drops duplicate rows from the dataset.\"\"\"\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_postcode(postcode):\n",
    "        \"\"\"Standardizes a postcode to the format 'SW1A 1AA'.\"\"\"\n",
    "        if isinstance(postcode, str):\n",
    "            postcode = postcode.strip().upper()\n",
    "            match = re.match(r'^([A-Z]{1,2}[0-9][A-Z0-9]?)(\\s*?)([0-9][A-Z]{2})$', postcode)\n",
    "            if match:\n",
    "                return f\"{match.group(1)} {match.group(3)}\"\n",
    "        return postcode\n",
    "\n",
    "    def preprocess_postcode(self, df):\n",
    "        \"\"\"Splits the 'postcode' column into 'postcodeSector' and 'postcodeDistrict'.\"\"\"\n",
    "        if \"postcode\" in df.columns:\n",
    "            df[\"postcodeSector\"] = df[\"postcode\"].str.split(\" \").str[0]\n",
    "            df[\"postcodeDistrict\"] = df[\"postcode\"].str.extract(r'^([A-Z]{1,2}\\d{1,2}\\s?\\d?)')[0]\n",
    "        return df\n",
    "\n",
    "    def merge_sector_data(self, df, sector_data):\n",
    "        \"\"\"Merges sector-level data.\"\"\"\n",
    "        if \"postcodeSector\" in df.columns:\n",
    "            df = df.merge(sector_data, on=\"postcodeSector\", how=\"left\")\n",
    "        return df\n",
    "\n",
    "    def merge_district_data(self, df, district_data):\n",
    "        \"\"\"Merges district-level data.\"\"\"\n",
    "        if \"postcodeDistrict\" in df.columns:\n",
    "            df = df.merge(district_data, on=\"postcodeDistrict\", how=\"left\")\n",
    "        return df\n",
    "\n",
    "    def increase_negative_for_log(self, df):\n",
    "        \"\"\"Shifts numeric columns with negative values for log scaling.\"\"\"\n",
    "        for col in df.select_dtypes(include=np.number).columns:\n",
    "            min_val = df[col].min()\n",
    "            if min_val <= 0:\n",
    "                df[col] += abs(min_val) + 0.001\n",
    "        return df\n",
    "\n",
    "    def lat_long_compute(self, df):\n",
    "        from flood_tool.geo import get_gps_lat_long_from_easting_northing  # Import the required method\n",
    "        # Extract latitude and longitude using the easting and northing columns\n",
    "        coordinates_lat_long = df.apply(\n",
    "            lambda row: get_gps_lat_long_from_easting_northing(row['easting'], row['northing']), axis=1\n",
    "    )\n",
    "        # Split the resulting tuples into separate latitude and longitude columns\n",
    "        df['latitude'] = coordinates_lat_long.apply(lambda x: x[0])\n",
    "        df['longitude'] = coordinates_lat_long.apply(lambda x: x[1])\n",
    "        # Drop easting and northing columns after conversion\n",
    "        df.drop(columns=['easting', 'northing'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def impute_watercourse_3d(self, df, lat_col=\"latitude\", lon_col=\"longitude\", elev_col=\"elevation\", watercourse_col=\"nearestWatercourse\"):\n",
    "        \"\"\"Imputes missing watercourse values using 3D Cartesian distance.\"\"\"\n",
    "        # Convert known and unknown locations to Cartesian coordinates\n",
    "        df[\"cartesian\"] = df.apply(lambda row: lat_lon_to_cartesian(row[lat_col], row[lon_col], row[elev_col]), axis=1)\n",
    "    \n",
    "        # Split data into known and unknown watercourses\n",
    "        known = df[df[watercourse_col].notna()]\n",
    "        unknown = df[df[watercourse_col].isna()]\n",
    "    \n",
    "        # Build KDTree for known Cartesian coordinates\n",
    "        known_coords = np.array(known[\"cartesian\"].tolist())\n",
    "        tree = KDTree(known_coords)\n",
    "    \n",
    "        # Query nearest neighbors for unknown Cartesian coordinates\n",
    "        unknown_coords = np.array(unknown[\"cartesian\"].tolist())\n",
    "        distances, indices = tree.query(unknown_coords, k=1)\n",
    "    \n",
    "        # Flatten indices array to 1D\n",
    "        indices = indices.flatten()\n",
    "    \n",
    "        # Impute missing values with the nearest neighbor's watercourse name\n",
    "        unknown[watercourse_col] = known.iloc[indices][watercourse_col].values\n",
    "    \n",
    "        # Combine known and updated unknown datasets\n",
    "        df = pd.concat([known, unknown]).drop(columns=[\"cartesian\"])\n",
    "    \n",
    "        return df\n",
    "    def impute_watercourse_3d(self, df, lat_col=\"latitude\", lon_col=\"longitude\", elev_col=\"elevation\", watercourse_col=\"nearestWatercourse\"):\n",
    "    \"\"\"Imputes missing watercourse values using 3D Cartesian distance.\"\"\"\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = [lat_col, lon_col, elev_col, watercourse_col]\n",
    "    for col in required_cols[:-1]:  # Exclude the categorical column for now\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    # Drop rows with NaN in coordinate columns\n",
    "    df = df.dropna(subset=[lat_col, lon_col, elev_col])\n",
    "    \n",
    "    # Convert coordinates to Cartesian\n",
    "    df[\"cartesian\"] = df.apply(lambda row: lat_lon_to_cartesian(row[lat_col], row[lon_col], row[elev_col]), axis=1)\n",
    "    \n",
    "    # Split data into known and unknown watercourses\n",
    "    known = df[df[watercourse_col].notna()]\n",
    "    unknown = df[df[watercourse_col].isna()]\n",
    "    \n",
    "    if known.empty:\n",
    "        raise ValueError(\"No known data points to build KDTree.\")\n",
    "    if unknown.empty:\n",
    "        print(\"No missing watercourse values to impute.\")\n",
    "        return df.drop(columns=[\"cartesian\"])  # Drop the helper column and return\n",
    "\n",
    "    # Build KDTree for known Cartesian coordinates\n",
    "    known_coords = np.array(known[\"cartesian\"].tolist())\n",
    "    tree = KDTree(known_coords)\n",
    "\n",
    "    # Query nearest neighbors for unknown Cartesian coordinates\n",
    "    unknown_coords = np.array(unknown[\"cartesian\"].tolist())\n",
    "    distances, indices = tree.query(unknown_coords, k=1)\n",
    "\n",
    "    # Safely update the unknown DataFrame\n",
    "    unknown = unknown.copy()  # Avoid SettingWithCopyWarning\n",
    "    unknown[watercourse_col] = known.iloc[indices.flatten()][watercourse_col].values\n",
    "\n",
    "    # Combine known and updated unknown datasets\n",
    "    df = pd.concat([known, unknown]).drop(columns=[\"cartesian\"])\n",
    "    return df\n",
    "    def handle_missing_numeric(self, df, numeric_col=\"medianPrice\"):\n",
    "        \"\"\"Handles missing numeric data by replacing with the median.\"\"\"\n",
    "        if numeric_col in df.columns:\n",
    "            median_value = df[numeric_col].median()\n",
    "            df[numeric_col].fillna(median_value, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def scale_numeric_features(self, df, numeric_columns, scaling_type=\"standard\"):\n",
    "        \"\"\"Scales numeric features using the specified scaler.\"\"\"\n",
    "        if scaling_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaling_type == \"minmax\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif scaling_type == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling_type. Use 'standard', 'minmax', or 'robust'.\")\n",
    "\n",
    "        numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "        if numeric_columns:\n",
    "            df[numeric_columns] = self.scaler.fit_transform(df[numeric_columns])\n",
    "        return df\n",
    "\n",
    "    def feature_engineering(self, df, sector_data=None):\n",
    "        \"\"\"Adds derived features.\"\"\"\n",
    "        if \"distanceToWatercourse\" in df.columns and \"elevation\" in df.columns:\n",
    "            df[\"proximity_risk\"] = df[\"distanceToWatercourse\"] / (df[\"elevation\"] + 1)\n",
    "\n",
    "        if sector_data is not None and \"population\" in sector_data.columns and \"households\" in sector_data.columns:\n",
    "            df[\"population_density\"] = sector_data[\"population\"] / sector_data[\"households\"]\n",
    "        return df\n",
    "    def interaction_general(self, df):\n",
    "        \"\"\"Generates interaction features based on categorical and numeric data.\"\"\"\n",
    "        # Initialize label encoders\n",
    "        label_encoder_soil = LabelEncoder()\n",
    "        label_encoder_watercourse = LabelEncoder()\n",
    "\n",
    "        # Encode 'soilType' and 'nearestWatercourse'\n",
    "        df['soilType_encoded'] = label_encoder_soil.fit_transform(df['soilType'])\n",
    "        df['nearestWatercourse_encoded'] = label_encoder_watercourse.fit_transform(df['nearestWatercourse'])\n",
    "\n",
    "        # Binning for elevation and distance to watercourse\n",
    "        bins_elevation = pd.qcut(df['elevation'], q=4, labels=[\"Low\", \"Mid\", \"High\", \"Very High\"])\n",
    "        bins_distance = pd.qcut(df['distanceToWatercourse'], q=4, labels=[\"Low\", \"Mid\", \"High\", \"Very High\"])\n",
    "\n",
    "        # Interaction terms\n",
    "        df['soilType/Elevation'] = df['soilType_encoded'].astype(str) + '/' + bins_elevation.astype(str)\n",
    "        df['distanceToWatercourse/nearestWatercourse'] = bins_distance.astype(str) + '/' + df['nearestWatercourse_encoded'].astype(str)\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        df.drop(columns=['soilType_encoded', 'nearestWatercourse_encoded'], inplace=True)\n",
    "\n",
    "        self.log.append(\"Added interaction terms and encoded features.\")\n",
    "        return df\n",
    "\n",
    "    def preprocess(self, df, sector_data=None, district_data=None, lat_col=\"latitude\", lon_col=\"longitude\", elev_col=\"elevation\",\n",
    "                   watercourse_col=\"nearestWatercourse\", numeric_col=\"medianPrice\", numeric_columns=None, scaling_type=\"standard\"):\n",
    "        \"\"\"Executes the complete preprocessing pipeline.\"\"\"\n",
    "        df = self.drop_duplicates(df)\n",
    "        df = self.preprocess_postcode(df)\n",
    "        df = self.merge_sector_data(df, sector_data)\n",
    "        df = self.merge_district_data(df, district_data)\n",
    "        df = self.lat_long_compute(df)\n",
    "        df = self.impute_watercourse_3d(df, lat_col, lon_col, elev_col, watercourse_col)\n",
    "        df = self.handle_missing_numeric(df, numeric_col)\n",
    "        df = self.feature_engineering(df, sector_data)\n",
    "        df = self.interaction_general(df)\n",
    "        df = self.scale_numeric_features(df, numeric_columns, scaling_type)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool\n",
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Add the parent directory of flood_tool to the Python path\n",
    "sys.path.append('/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee')\n",
    "\n",
    "# Test the import\n",
    "from flood_tool.geo import get_gps_lat_long_from_easting_northing\n",
    "print(\"Import successful!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing Test\n",
    "#Load all files\n",
    "postcodes_missing = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/example_data/postcodes_missing_data.csv\")\n",
    "postcodes_labelled = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/postcodes_labelled.csv\")\n",
    "postcodes_unlabelled = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/example_data/postcodes_unlabelled.csv\")\n",
    "sector_data = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/sector_data.csv\")\n",
    "station_data = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/stations.csv\")\n",
    "typical_day = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/example_data/typical_day.csv\")\n",
    "wet_day = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/example_data/wet_day.csv\")\n",
    "district_data = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/district_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "data must be of shape (n, m), where there are n points of dimension m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m FloodRiskPreprocessingPipeline()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the preprocessing pipeline\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_processed \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpreprocess(\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m=\u001b[39mpostcodes_labelled,\n\u001b[1;32m      7\u001b[0m     sector_data\u001b[38;5;241m=\u001b[39msector_data,\n\u001b[1;32m      8\u001b[0m     district_data\u001b[38;5;241m=\u001b[39mdistrict_data,\n\u001b[1;32m      9\u001b[0m     lat_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     lon_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     elev_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     watercourse_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearestWatercourse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     numeric_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedianPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     numeric_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistanceToWatercourse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedianPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     scaling_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Display processed DataFrame\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_processed\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[27], line 155\u001b[0m, in \u001b[0;36mFloodRiskPreprocessingPipeline.preprocess\u001b[0;34m(self, df, sector_data, district_data, lat_col, lon_col, elev_col, watercourse_col, numeric_col, numeric_columns, scaling_type)\u001b[0m\n\u001b[1;32m    153\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_district_data(df, district_data)\n\u001b[1;32m    154\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlat_long_compute(df)\n\u001b[0;32m--> 155\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpute_watercourse_3d(df, lat_col, lon_col, elev_col, watercourse_col)\n\u001b[1;32m    156\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_missing_numeric(df, numeric_col)\n\u001b[1;32m    157\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_engineering(df, sector_data)\n",
      "Cell \u001b[0;32mIn[27], line 76\u001b[0m, in \u001b[0;36mFloodRiskPreprocessingPipeline.impute_watercourse_3d\u001b[0;34m(self, df, lat_col, lon_col, elev_col, watercourse_col)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Build KDTree for known Cartesian coordinates\u001b[39;00m\n\u001b[1;32m     75\u001b[0m known_coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(known[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcartesian\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 76\u001b[0m tree \u001b[38;5;241m=\u001b[39m KDTree(known_coords)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Query nearest neighbors for unknown Cartesian coordinates\u001b[39;00m\n\u001b[1;32m     79\u001b[0m unknown_coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(unknown[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcartesian\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deluge/lib/python3.12/site-packages/scipy/spatial/_kdtree.py:360\u001b[0m, in \u001b[0;36mKDTree.__init__\u001b[0;34m(self, data, leafsize, compact_nodes, copy_data, balanced_tree, boxsize)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKDTree does not work with complex data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# Note KDTree has different default leafsize from cKDTree\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data, leafsize, compact_nodes, copy_data,\n\u001b[1;32m    361\u001b[0m                  balanced_tree, boxsize)\n",
      "File \u001b[0;32m_ckdtree.pyx:560\u001b[0m, in \u001b[0;36mscipy.spatial._ckdtree.cKDTree.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: data must be of shape (n, m), where there are n points of dimension m"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = FloodRiskPreprocessingPipeline()\n",
    "\n",
    "# Run the preprocessing pipeline\n",
    "df_processed = pipeline.preprocess(\n",
    "    df=postcodes_labelled,\n",
    "    sector_data=sector_data,\n",
    "    district_data=district_data,\n",
    "    lat_col=\"latitude\",\n",
    "    lon_col=\"longitude\",\n",
    "    elev_col=\"elevation\",\n",
    "    watercourse_col=\"nearestWatercourse\",\n",
    "    numeric_col=\"medianPrice\",\n",
    "    numeric_columns=[\"distanceToWatercourse\", \"elevation\", \"medianPrice\"],\n",
    "    scaling_type=\"standard\"\n",
    ")\n",
    "\n",
    "# Display processed DataFrame\n",
    "print(df_processed.head())  # Displays the first 5 rows of the processed DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = postcodes_labelled \n",
    "print(\"DataFrame Info:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nCategorical Columns:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "print(\"\\nNumeric Columns:\")\n",
    "print(numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns + numeric_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Column '{col}' is missing from the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataset and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "postcodes_labelled = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/postcodes_labelled.csv\")\n",
    "postcodes_unlabelled = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/example_data/postcodes_unlabelled.csv\")\n",
    "sector_data = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/sector_data.csv\")\n",
    "station_data = pd.read_csv(\"/Users/yixuanyan/Desktop/edsml/Term1/jubliee/ads-deluge-jubilee/flood_tool/resources/stations.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Define categorical and numeric columns\n",
    "categorical_columns = ['soilType', 'localAuthority', 'nearestWatercourse']\n",
    "numeric_columns = ['easting','northing','elevation', 'distanceToWatercourse', 'medianPrice','riskLabel','medianPrice','historicallyFlooded']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FloodRiskPreprocessingPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_labelled = pipeline.preprocess(\n",
    "    df=postcodes_labelled,\n",
    "    categorical_columns=categorical_columns,\n",
    "    numeric_columns=numeric_columns,\n",
    "    scaling_type=\"standard\",  # Use \"minmax\" for MinMaxScaler\n",
    "    imputation_method=\"median\",  # Options: \"median\", \"mean\"\n",
    "    sector_data=sector_data,\n",
    "    station_data=station_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[categorical_columns].head())\n",
    "print(df[categorical_columns].dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deluge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
