{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is used to predict the flood risk of a location based on regression model (XGBoostregressor)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor\n",
    "from flood_tool.geo import get_gps_lat_long_from_easting_northing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('flood_tool/resources/postcodes_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/qf5v72y97y709qmzdz43p2f00000gn/T/ipykernel_51967/3881437295.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['nearestWatercourse'].fillna('NoStreamNear', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "coordinates_lat = get_gps_lat_long_from_easting_northing(data['easting'], data['northing'])\n",
    "coordinates_df = pd.DataFrame({\n",
    "    'Latitude': coordinates_lat[0],\n",
    "    'Longitude': coordinates_lat[1]\n",
    "})\n",
    "data = pd.concat([data, coordinates_df], axis=1)\n",
    "data.drop(columns=['easting', 'northing'], inplace=True)\n",
    "\n",
    "data['log_medianPrice'] = np.log(data['medianPrice'])\n",
    "\n",
    "# Create bins for log-transformed medianPrice based on the description of log median price\n",
    "bins = [-np.inf, 11.9, 14.2, np.inf]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "# Apply log transformation and binning\n",
    "data['price_category'] = pd.cut(data['log_medianPrice'], bins=bins, labels=labels)\n",
    "# Display the first few rows to verify\n",
    "data[['medianPrice', 'log_medianPrice', 'price_category']].head()\n",
    "# postcodes_labelled.isnull().sum()\n",
    "\n",
    "#data['nearestWatercourse'].nunique() # 1146 + Nan which is No nearest WaterCourse\n",
    "data['nearestWatercourse'].fillna('NoStreamNear', inplace=True)\n",
    "\n",
    "## Define bins and labels\n",
    "elevation_bins = [-10, 0, 40, 80, np.inf]\n",
    "elevation_labels = ['BelowSeaLevel', 'Low', 'Medium', 'High']\n",
    "# Apply binning to the elevation column\n",
    "data['elevation_category'] = pd.cut(data['elevation'], bins=elevation_bins, labels=elevation_labels)\n",
    "\n",
    "# Define bins and labels\n",
    "distance_bins = [0, 630, 1090, 1840, np.inf]\n",
    "distance_labels = ['Very Close', 'Close', 'Moderate', 'Far']\n",
    "\n",
    "# Apply binning to the distanceToWatercourse column\n",
    "data['distanceToWatercourse_category'] = pd.cut(data['distanceToWatercourse'], bins=distance_bins, labels=distance_labels)\n",
    "\n",
    "bins = [-np.inf, 11.9, 14.2, np.inf]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "# Apply log transformation and binning\n",
    "data['log_medianPrice'] = np.log(data['medianPrice'])\n",
    "data['price_category'] = pd.cut(data['log_medianPrice'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log1p(x+np.abs(data['elevation'].min()))\n",
    "\n",
    "numeric_features = ['elevation', 'distanceToWatercourse', 'soilType_encoded', 'nearestWatercourse_encoded', 'localAuthority_encoded']\n",
    "numeric_remained = ['Latitude', 'Longitude']\n",
    "categorical_features = []\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(log_transform, validate=True)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "numeric_remained_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('num_rem', numeric_remained_transformer, numeric_remained),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "standard_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelEncode (outside of pipeline)\n",
    "label_encoder = LabelEncoder()\n",
    "data['localAuthority_encoded'] = label_encoder.fit_transform(data['localAuthority'])\n",
    "data['soilType_encoded'] = label_encoder.fit_transform(data['soilType'])\n",
    "data['nearestWatercourse_encoded'] = label_encoder.fit_transform(data['nearestWatercourse'])\n",
    "\n",
    "X = data[['elevation', 'soilType_encoded', 'nearestWatercourse_encoded', 'distanceToWatercourse', 'Latitude', 'Longitude', 'localAuthority_encoded']]\n",
    "y = data['riskLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.8547\n",
      "Root Mean Squared Error (RMSE): 0.9245\n",
      "R-squared (R²): 0.3800\n"
     ]
    }
   ],
   "source": [
    "# enconde y to 0, 1, 2, 3, 4, 5, 6\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_transformed = standard_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = standard_pipeline.transform(X_test)\n",
    "\n",
    "# imbalanced data handling (SMOTE and ADASYN)\n",
    "# SMOTE: Generate a few class samples by interpolation.\n",
    "# ADASYN: Add noise around a few class samples to produce a more diverse sample.\n",
    "smote = SMOTE(sampling_strategy={5: 5000, 6: 2000}, random_state=42)\n",
    "adasyn = ADASYN(sampling_strategy={5: 6000, 6: 3000}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# data augmentation for class 6 and 7\n",
    "X_train_class6 = X_train_transformed[y_train == 5]\n",
    "X_train_class7 = X_train_transformed[y_train == 6]\n",
    "noise6 = np.random.normal(0, 0.01, X_train_class6.shape)\n",
    "noise7 = np.random.normal(0, 0.01, X_train_class7.shape)\n",
    "X_augmented = np.vstack([X_train_class6 + noise6, X_train_class7 + noise7])\n",
    "y_augmented = np.hstack([np.full(X_train_class6.shape[0], 5), np.full(X_train_class7.shape[0], 6)])\n",
    "\n",
    "X_train_resampled = np.vstack([X_train_resampled, X_augmented])\n",
    "y_train_resampled = np.hstack([y_train_resampled, y_augmented])\n",
    "\n",
    "# calculate class weights based on the quantity of samples\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "XGboost = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "XGboost.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# predict the test set\n",
    "y_pred_continuous = XGboost.predict(X_test_transformed)\n",
    "\n",
    "# round the continuous predictions to the nearest integer\n",
    "y_pred_encoded = np.round(y_pred_continuous).astype(int)\n",
    "\n",
    "# make sure the predictions are within the range of the classes\n",
    "y_pred_encoded = np.clip(y_pred_encoded, 0, len(label_encoder.classes_) - 1)\n",
    "\n",
    "# decode the predictions\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# evaluate the model\n",
    "mse = mean_squared_error(y_test_original, y_pred)  # MSE: Mean Squared Error\n",
    "rmse = np.sqrt(mse)  # RMSE: Root Mean Squared Error\n",
    "r2 = r2_score(y_test_original, y_pred)  # R^2: R-squared\n",
    "\n",
    "# print results\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R-squared (R²): {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/npp2024/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.9050566973395904, 'learning_rate': 0.2701216955740311, 'max_depth': 14, 'n_estimators': 490, 'reg_alpha': 5.015162946871996, 'reg_lambda': 7.9829517896677515, 'subsample': 0.8249819653888826}\n",
      "Mean Squared Error (MSE): 0.8947\n",
      "Root Mean Squared Error (RMSE): 0.9459\n",
      "R-squared (R²): 0.3511\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),       # 树的数量\n",
    "    'max_depth': randint(3, 15),             # 树的最大深度\n",
    "    'learning_rate': uniform(0.01, 0.3),    # 学习率\n",
    "    'subsample': uniform(0.5, 0.5),         # 随机采样比例\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # 每棵树使用的特征比例\n",
    "    'reg_alpha': uniform(0, 10),            # L1 正则化\n",
    "    'reg_lambda': uniform(0, 10)            # L2 正则化\n",
    "}\n",
    "\n",
    "# create an XGBoost model\n",
    "XGboost = XGBRegressor(random_state=42, eval_metric='rmse')\n",
    "\n",
    "# define the random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGboost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # 随机搜索迭代次数\n",
    "    scoring='neg_mean_squared_error',  # 使用负均方误差作为得分\n",
    "    cv=5,  # 交叉验证折数\n",
    "    verbose=0,  # 显示搜索过程\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # 使用所有可用 CPU 进行并行计算\n",
    ")\n",
    "\n",
    "# conduct the random search\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# use the best model to make predictions\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_continuous = best_model.predict(X_test_transformed)\n",
    "\n",
    "# round the continuous predictions to the nearest integer\n",
    "y_pred_encoded = np.round(y_pred_continuous).astype(int)\n",
    "\n",
    "# make sure the predictions are within the range of the classes\n",
    "y_pred_encoded = np.clip(y_pred_encoded, 0, len(label_encoder.classes_) - 1)\n",
    "\n",
    "# decode the predictions\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# evaluate the model\n",
    "mse = mean_squared_error(y_test_original, y_pred)  # MSE: Mean Squared Error\n",
    "rmse = np.sqrt(mse)  # RMSE: Root Mean Squared Error\n",
    "r2 = r2_score(y_test_original, y_pred)  # R^2: R-squared\n",
    "\n",
    "# print results\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R-squared (R²): {r2:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npp2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
